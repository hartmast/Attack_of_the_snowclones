if(!is.element("concordances", installed.packages())) {
devtools::install_github("hartmast/concordances")
}
# if this doesn't work, check sfla.ch for the package
if(!is.element("collostructions", installed.packages())) {
install.packages("https://sfla.ch/wp-content/uploads/2021/02/collostructions_0.2.0.tar.gz", repos = NULL)
}
# install "concordances" if not yet installed
if(!is.element("concordances", installed.packages())) {
devtools::install_github("hartmast/concordances", ref = "f4ca785
")
}
# install "wordVectors" if not yet installed
if(!is.element("wordVectors", installed.packages())) {
devtools::install_github("bmschmidt/wordVectors")
}
# load "concordances"
library(concordances)
library(tidyverse)
library(readxl)
library(data.table)
library(kableExtra)
library(collostructions)
library(wordVectors)
library(vroom)
library(ggrepel)
library(cluster)
library(patchwork)
# Chunk 5: import
d <- getNSE("../data/mother_of_all.xml", xml = TRUE, context_tags = FALSE, verbose = FALSE)
# Chunk 6: anno
# write_excel_csv(d, "mother_of_all_ENCOW.csv")
d <- read_xlsx("../data/mother_of_all_ENCOW.xlsx")
# Chunk 7: dw
d <- filter(d, keep == "y")
# Chunk 8
# frequency table
d_tbl <- d %>% select(lemma) %>% table %>% as_tibble() %>% rename(c(Freq = "n")) %>% arrange(desc(Freq))
# overview table
tibble(
# types
Types = nrow(d_tbl),
# tokens
Tokens = sum(d_tbl$Freq),
# hapax legomena
"Hapax Legomena" = length(which(d_tbl$Freq==1))
) %>% kbl() %>% kable_material(c("striped", "hover"))
# Chunk 13
# re-import
d_tbl <- read_csv("../data/mother_of_all_with_encow_frequencies.csv")
# Chunk 14: excludethis
# omit one case where corpus frequency is
# smaller than cxn frequency
d_tbl <- subset(d_tbl, d_tbl[,2] <= d_tbl[,3])
# Chunk 15: collex
# sum(encow$Freq): 1805183579
# perform collexeme analysis
collex(as.data.frame(d_tbl), corpsize = 1805183579) %>% kbl() %>%
kable_material(c("striped", "hover")) %>% scroll_box(width = "800px", height = "200px")
# Chunk 16
# read data
moa <- read_xlsx("../data/motherofall_COCA.xlsx")
# full frequency list cannot be shared publicly
# for license reasons, hence we work with the
# list containing only the lemmas occurring in
# mother of all
#coca <- fread("../coca_2017_lemma_frequency_list.txt", quote = "")
coca <- fread("../data/coca_moa_lemma_frequencies.csv")
# replace whitespaces in column names
colnames(moa) <- gsub(" ", "_", colnames(moa))
colnames(coca) <- c("No", "Lemma", "Freq")
# omit false hits
moa <- subset(moa, keep=="y")
# types, tokens, and hapax legomena overall
moa_tbl1 <- moa %>% select(lemma) %>% table %>% as_tibble() %>% rename(c(Freq = "n") ) %>% arrange(desc(Freq))
tibble(
Tokens = sum(moa_tbl1$Freq),
Types = nrow(moa_tbl1),
"Hapax Legomena" = length(which(moa_tbl1$Freq==1))
)
# generate input for collostructional analysis
moa_lemmas <- moa$lemma %>% table %>% sort(decreasing = T) %>% as.data.frame(stringsAsFactors = F)
colnames(moa_lemmas) <- c("Lemma", "Freq_in_cxn")
all_lemmas <- coca[,Lemma, Freq]
setcolorder(all_lemmas, c("Lemma", "Freq"))
collex_input <- join.freqs(moa_lemmas, as.data.frame(all_lemmas), all = F)
colnames(collex_input) <- c("Lemma", "cxn_freq", "cxn_all")
collex_input <- subset(collex_input, cxn_all != 0)
collex(collex_input, corpsize = sum(coca$Freq)) # %>% write_excel_csv("simple_collexeme_analysis.csv")
# relative frequency ------------------------------------------------------
# get COCA frequencies
coca_freq <- read_xlsx("../data/COCA2017_total_frequencies.xlsx")
# tabulate mother frequency
moa_tbl <- table(moa$Year) %>% as.data.frame(stringsAsFactors = F)
colnames(moa_tbl) <- c("YEAR", "Freq")
moa_tbl$YEAR <- as.numeric(moa_tbl$YEAR)
moa_tbl <- left_join(coca_freq, moa_tbl, by = "YEAR")
moa_tbl$pmw <- (moa_tbl$Freq / moa_tbl$TOTAL) * 1e06
# plot
# png("mother_of_all_coca_freq.png", width = 6.5, height = 5, un = "in", res = 300)
plot(moa_tbl$YEAR, moa_tbl$pmw, pch = 20, col = "blue",
ylab = "Frequency per million words", xlab = "Year",
main = expression(paste("[", italic("mother of all"), " X], COCA")))
abline(lm(moa_tbl$pmw ~ moa_tbl$YEAR), lty = 2, col = "darkgrey")
# dev.off()
# types per decade
moa_types <- moa %>% group_by(Decade) %>% summarise(
types = length(unique(lemma)),
n = n()
)
# types per year
moa_types_year <- moa %>% group_by(Year) %>% summarise(
types = length(unique(lemma))
)
# add to table with total frequencies
moa_tbl <- left_join(moa_types_year, moa_tbl, by = c("Year" = "YEAR"))
moa_tbl <- rename(moa_tbl, "YEAR" = "Year")
moa_tbl$types_pmw <- (moa_tbl$types / moa_tbl$TOTAL) * 1e6
# coca_freq per dcade
coca_freq$Decade <- floor(coca_freq$YEAR/10)*10
coca_freq_decade <- coca_freq %>% group_by(Decade) %>% summarise(
n = sum(TOTAL)
)
# types per decade
moa_types <- left_join(moa_types, coca_freq_decade)
moa_types$rel <- moa_types$types / moa_types$n
moa_types$rel %>% plot
moa_types$ttr <- moa_types$types / moa_types$n
# distribution of hapaxes
hapaxes <- table(moa$lemma) %>% as.data.frame %>% filter(Freq==1) %>% select(Var1) %>% as.vector
hapaxes <- as.character(hapaxes$Var1)
moa$hapax <- ifelse(moa$lemma %in% hapaxes, "y", "n")
moa_hapaxes <- moa %>% group_by(Decade) %>% summarise(
hapaxes = length(which(hapax=="y")),
n = n()
)
# plot potential productivity
moa_hapaxes$pp <- moa_hapaxes$hapaxes / moa_hapaxes$n
par(mfrow = c(1,3))
# png("types_tokens_mother_COCA.png", width = 12, height = 4, un = "in", res = 300)
par(mfrow=c(1,3))
par(mar = c(5.1, 5.1, 5.1, 2.1))
plot(moa_tbl$YEAR, moa_tbl$pmw, pch = 20, col = "blue",
ylab = "Frequency per million words", xlab = "Year",
main = expression(paste(bold("["), bolditalic("mother of all"), bold(" X], COCA"))),
cex = 2, cex.lab = 2, cex.axis=1.5)
abline(lm(moa_tbl$pmw ~ moa_tbl$YEAR), lty = 2, col = "darkgrey", lwd = 2)
points(moa_tbl$YEAR, moa_tbl$types_pmw, col = rgb(1,0,0,.5), pch = 18, cex = 1.3)
plot(moa_types$Decade, moa_types$ttr,
type = "b", pch=18,
ylab = "Types / Tokens", xlab = "Decade",
main = "Type-Token Ratio, COCA",
lwd = 2, col = "blue", cex = 2, cex.lab = 2, cex.axis=2,
xaxt = "n"
)
axis(1, at = c(1990, 2000, 2010), cex.axis=2)
plot(moa_hapaxes$Decade, moa_hapaxes$pp, type = "b", pch=18,
ylab = "Proportion hapaxes", xlab = "Decade",
main = "Potential productivity \n (proportion of hapax legomena), COCA",
lwd = 2, col = "blue", cex = 2, cex.lab = 2, cex.axis=2, xaxt = "n")
axis(1, at = c(1990, 2000, 2010), cex.axis=2)
# dev.off()
par(mar = c(5.1, 4.1, 4.1, 2.1))
par(mfrow=c(1,1))
# Chunk 17
(p1 <- collex(collex_input, corpsize = sum(coca$Freq), am = "odds") %>% ggplot(aes(x = log1p(OBS), y = log1p(COLL.STR.ODDS), label = COLLEX, col = log1p(OBS))) + geom_text() + theme_bw() + xlab("Log odds ratio") + ylab("Log Frequency") + scale_color_continuous(low = rgb(0,.7,1,.4), high = "black") + guides(col = 'none') + ggtitle("COCA") +theme(plot.title = element_text(face = "bold", hjust = 0.5)))
(p2 <- collex(as.data.frame(d_tbl), corpsize = 1805183579, am = "odds") %>% ggplot(aes(x = log1p(OBS), y = log1p(COLL.STR.ODDS), label = COLLEX, col = log1p(OBS))) + geom_text() + theme_bw() + xlab("Log odds ratio") + ylab("Log Frequency") + scale_color_continuous(low = rgb(0,.7,1,.4), high = "black") + guides(col = 'none') + ggtitle("ENCOW") + theme(plot.title = element_text(face = "bold", hjust = 0.5)))
p1 | p2
# ggsave("collex_moa_coca_encow.png", height = 7, width = 13)
# Chunk 19: distsem
# import the model
model <- readRDS("/Users/stefanhartmann/sciebo/Projekte/snowclones/word2vec/model.Rds")
# matrix of terms occurring in [mother of all X]'s open slot
cosine_dist_matrix <- cosineDist(model[[moa_lemmas$Lemma, average = FALSE]], model[[moa_lemmas$Lemma, average = FALSE]])
# multidimensional scaling
cosine_dists <- cosine_dist_matrix %>% cmdscale() %>% as.data.frame() %>% rownames_to_column() %>% setNames(c("lemma", "V1", "V2"))
# alternative: t-SNE
cosine_rtsne <- cosine_dist_matrix %>% Rtsne::Rtsne()
# we use Partitioning Around Medioids (PAM) to
# identify a small number of clusters (here: 3).
# As the results are not really meaningful, we have
# refrained from including it in the final analysis though.
# get PAM clusters
# for(i in 2:10) {
#   print(pam(cosine_dists, i)$silinfo$avg.width)
# }
#
pams <- pam(cosine_dists, 3)$clustering
# add frequency information
moa_freqs <- moa$lemma %>% table %>% as_tibble() %>% setNames(c("lemma", "n"))
# combine with MDS results
cosine_dists <- left_join(cosine_dists, moa_freqs)
# add log frequency
cosine_dists$LogFreq <- log1p(cosine_dists$n)
# 3 clusters
cosine_dists$clusters <- pams
# add Rtsne
cosine_dists <- cbind(cosine_dists, setNames(as.data.frame(cosine_rtsne$Y), c("dim1", "dim2")))
# visualize
# add one column that only serves to increase the font
# size of the remaining items (only for print version)
cosine_dists <- rbind(cosine_dists,
data.frame(lemma = "",
V1 = 0,
V2 = 0,
n = 0,
LogFreq = 0.3,
clusters = 1,
dim1 = 0, dim2 = 0))
set.seed(1994)
ggplot(cosine_dists, aes(x = V1, y = V2, label = lemma, size = LogFreq, col = factor(clusters))) +
geom_text_repel(max.overlaps = 15) +
guides(col = "none", size = "none") + theme_bw() +
# theme(axis.text = element_text(size = 18)) +
# theme(axis.title = element_text(size = 18)) +
# theme(strip.text = element_text(size = 18)) +
# theme(legend.title = element_text(size = 18, face = "bold")) +
# theme(text = element_text(size = 18)) +
scale_color_viridis_d() + ylab("dim2") + xlab("dim1")
# ggsave("distsem_moa_word2vec.png", width = 7, height = 6, dpi=500)
# use RTsne instead
set.seed(1994)
ggplot(cosine_dists, aes(x = dim1, y = dim2, label = lemma, size = LogFreq)) +
geom_text_repel(max.overlaps = 15) +
guides(size = "none") + theme_bw() +
# theme(axis.text = element_text(size = 18)) +
# theme(axis.title = element_text(size = 18)) +
# theme(strip.text = element_text(size = 18)) +
# theme(legend.title = element_text(size = 18, face = "bold")) +
# theme(text = element_text(size = 18)) +
scale_color_viridis_d() + ylab("dim2") + xlab("dim1")
# ggsave("distsem_moa_word2vec_tsne.png", width = 7, height = 6, dpi=500)
# sum(encow$Freq): 1805183579
# perform collexeme analysis
collex(as.data.frame(d_tbl), corpsize = 1805183579) %>% kbl() %>%
kable_material(c("striped", "hover")) %>% scroll_box(width = "800px", height = "200px")
collex(as.data.frame(d_tbl), corpsize = 1805183579)
collex(as.data.frame(d_tbl), corpsize = 1805183579, am = "cramersv")
collex(as.data.frame(d_tbl), corpsize = 1805183579, am = "Cramersv")
collex(as.data.frame(d_tbl), corpsize = 1805183579, am = "cramersV")
select(collex(as.data.frame(d_tbl), corpsize = 1805183579, am = "cramersV"), COLLEX, COLL.STR.CRAMERSV)
left_join(collex(as.data.frame(d_tbl), corpsize = 1805183579),
select(collex(as.data.frame(d_tbl), corpsize = 1805183579, am = "cramersV"), COLLEX, COLL.STR.CRAMERSV))
left_join(collex(as.data.frame(d_tbl), corpsize = 1805183579),
select(collex(as.data.frame(d_tbl), corpsize = 1805183579, am = "cramersV"), COLLEX, COLL.STR.CRAMERSV)) %>%  kbl() %>%
kable_material(c("striped", "hover")) %>% scroll_box(width = "800px", height = "200px")
# plot
# png("mother_of_all_coca_freq.png", width = 6.5, height = 5, un = "in", res = 300)
plot(moa_tbl$YEAR, moa_tbl$pmw, pch = 20, col = "blue",
ylab = "Frequency per million words", xlab = "Year",
main = "Token and type frequencies")
#main = expression(paste("[", italic("mother of all"), " X], COCA")))
abline(lm(moa_tbl$pmw ~ moa_tbl$YEAR), lty = 2, col = "darkgrey")
plot(moa_tbl$YEAR, moa_tbl$pmw, pch = 20, col = "blue",
ylab = "Frequency per million words", xlab = "Year",
main = "Token and type frequencies",
#main = expression(paste(bold("["), bolditalic("mother of all"), bold(" X], COCA"))),
cex = 2, cex.lab = 2, cex.axis=1.5)
abline(lm(moa_tbl$pmw ~ moa_tbl$YEAR), lty = 2, col = "darkgrey", lwd = 2)
par(mfrow = c(1,3))
png("types_tokens_mother_COCA.png", width = 12, height = 4, un = "in", res = 300)
par(mfrow=c(1,3))
par(mar = c(5.1, 5.1, 5.1, 2.1))
plot(moa_tbl$YEAR, moa_tbl$pmw, pch = 20, col = "blue",
ylab = "Frequency per million words", xlab = "Year",
main = "Token and type frequencies",
#main = expression(paste(bold("["), bolditalic("mother of all"), bold(" X], COCA"))),
cex = 2, cex.lab = 2, cex.axis=1.5)
abline(lm(moa_tbl$pmw ~ moa_tbl$YEAR), lty = 2, col = "darkgrey", lwd = 2)
points(moa_tbl$YEAR, moa_tbl$types_pmw, col = rgb(1,0,0,.5), pch = 18, cex = 1.3)
plot(moa_types$Decade, moa_types$ttr,
type = "b", pch=18,
ylab = "Types / Tokens", xlab = "Decade",
main = "Type-Token Ratio, COCA",
lwd = 2, col = "blue", cex = 2, cex.lab = 2, cex.axis=2,
xaxt = "n"
)
axis(1, at = c(1990, 2000, 2010), cex.axis=2)
plot(moa_hapaxes$Decade, moa_hapaxes$pp, type = "b", pch=18,
ylab = "Proportion hapaxes", xlab = "Decade",
main = "Potential productivity \n (proportion of hapax legomena), COCA",
lwd = 2, col = "blue", cex = 2, cex.lab = 2, cex.axis=2, xaxt = "n")
axis(1, at = c(1990, 2000, 2010), cex.axis=2)
dev.off()
par(mar = c(5.1, 4.1, 4.1, 2.1))
par(mfrow=c(1,1))
?collex
citation("collostructions")
?collex
collex(collex_input, corpsize = sum(coca$Freq), delta.p = T) # %>% write_excel_csv("simple_collexeme_analysis.csv")
left_join(collex(as.data.frame(d_tbl), corpsize = 1805183579),
select(collex(as.data.frame(d_tbl), corpsize = 1805183579, am = "cramersV"), COLLEX, COLL.STR.CRAMERSV), delta.p = T) %>%  kbl() %>%
kable_material(c("striped", "hover")) %>% scroll_box(width = "800px", height = "200px")
left_join(collex(as.data.frame(d_tbl), corpsize = 1805183579, delta.p = TRUE),
select(collex(as.data.frame(d_tbl), corpsize = 1805183579, am = "cramersV"), COLLEX, COLL.STR.CRAMERSV)) %>%  kbl() %>%
kable_material(c("striped", "hover")) %>% scroll_box(width = "800px", height = "200px")
collex(collex_input, corpsize = sum(coca$Freq), am = "odds")
collex(collex_input, corpsize = sum(coca$Freq), am = "cramersV")
collex(collex_input, corpsize = sum(coca$Freq), am = "odds")
select(collex(collex_input, corpsize = sum(coca$Freq), am = "odds"), COLLEX, COLL.STR.ODDS)
left_join(collex(collex_input, corpsize = sum(coca$Freq), delta.p = T),
select(collex(collex_input, corpsize = sum(coca$Freq), am = "odds"), COLLEX, COLL.STR.ODDS))
library(DT)
left_join(collex(as.data.frame(d_tbl), corpsize = 1805183579, delta.p = TRUE),
select(collex(as.data.frame(d_tbl), corpsize = 1805183579, am = "cramersV"), COLLEX, COLL.STR.CRAMERSV)) %>% DT:datatable()  #kbl() %>%
left_join(collex(as.data.frame(d_tbl), corpsize = 1805183579, delta.p = TRUE),
select(collex(as.data.frame(d_tbl), corpsize = 1805183579, am = "cramersV"), COLLEX, COLL.STR.CRAMERSV)) %>% DT::datatable()  #kbl() %>%
left_join(collex(collex_input, corpsize = sum(coca$Freq), delta.p = T),
select(collex(collex_input, corpsize = sum(coca$Freq), am = "odds"), COLLEX, COLL.STR.ODDS)) %>% DT::datatable()
?collex
# Chunk 1: setup
knitr::opts_chunk$set(echo = TRUE)
# Chunk 2: sessionInfo
sessionInfo()
# Chunk 3: pkg
# install CRAN packages (if not yet installed)
sapply(c("data.table", "tidyverse", "devtools", "readxl", "kableExtra", "ngram", "networkD3", "igraph", "network", "patchwork", "koRpus", "pbapply", "tidytext", "cluster", "ggrepel", "animation", "kableExtra", "DT", "vroom", "Rtsne"), function(x) if(!is.element(x, installed.packages())) install.packages(x, dependencies = T, repos = "http://cran.us.r-project.org"))
# install non-CRAN packages (if not yet installed)
if(!is.element("concordances", installed.packages())) {
devtools::install_github("hartmast/concordances")
}
# install non-CRAN packages (if not yet installed)
if(!is.element("wizard", installed.packages())) {
devtools::install_github("hartmast/wizard")
}
# if this doesn't work, check sfla.ch for the package
if(!is.element("collostructions", installed.packages())) {
install.packages("https://sfla.ch/wp-content/uploads/2021/02/collostructions_0.2.0.tar.gz", repos = NULL)
}
# install "wordVectors" if not yet installed
if(!is.element("wordVectors", installed.packages())) {
devtools::install_github("bmschmidt/wordVectors")
}
# load packages
library(readxl)
library(tidyverse)
library(ngram)
library(networkD3)
library(igraph)
library(network)
library(patchwork)
library(koRpus)
library(pbapply)
library(tidytext)
library(cluster)
library(ggrepel)
library(animation)
library(kableExtra)
library(DT)
library(collostructions) # available at sflach.ch
library(concordances) #available at github.com/hartmast/concordances
library(wizard) # available at github.com/hartmast/wizard
library(vroom)
library(wordVectors)
# Chunk 4: hlp
# logarithmize and return 0 instead of Inf if x==0
log0 <- function(x) {
x <- ifelse(x == 0, 0, log(x))
return(x)
}
# function for "prettyfying" df output
# inspired by https://github.com/rmcelreath/rethinking/blob/d0978c7f8b6329b94efa2014658d750ae12b1fa2/R/utilities.r
pretty_df <- function(df) {
# function for rounding
round_this <- function(x, digits = 2) ifelse(x < 1, signif(x, digits = digits), round(x, digits = 2))
# function for getting prettyfied dataframe
df_pretty <- as.data.frame(lapply(1:length(df),
function(i) if(!class(df[[i]]) %in% c("character", "factor"))
{
round_this(df[[i]])
} else {
return(df[[i]])
})
)
# set names to original names
colnames(df_pretty) <- colnames(df)
return(df_pretty)
}
# search for entire words
grepw <- function(pattern, x, perl = F, ...) {
grep(paste("^", pattern, "$", sep="", collapse=""), x, perl = perl, ...)
}
# Chunk 5: readdata
d <- read_xlsx("../data/ENCOW_x_is_the_new_y_without_false_hits.xlsx")
# Chunk 6: datawrangling
# exclude false hits ------------------------------------------------------
d <- filter(d, keep == "y")
# add wordcount for x and y lemmas ----------------------------------------
d$wordcount_x <- sapply(1:nrow(d), function(i) wordcount(trimws(d$Lemma_x[i])))
d$wordcount_y <- sapply(1:nrow(d), function(i) wordcount(trimws(d$Lemma_y[i])))
# get heads of compounds and phrases --------------------------------------
# find instances in which there are words
# written entirely in uppercase (= our way of
# marking heads in the data, unless in the case of
# right-hand heads)
# empty columns for heads
d$head_x <- NA; d$head_y <- NA
# add wordcount for x and y lemmas
d$wordcount_x <- sapply(1:nrow(d), function(i) wordcount(trimws(d$Lemma_x[i])))
d$wordcount_y <- sapply(1:nrow(d), function(i) wordcount(trimws(d$Lemma_y[i])))
# get heads
for(i in 1:nrow(d)) {
if(d$wordcount_x[i]>1) {
if(d$pos_x[i]!="NE" & grepl("[A-Z]{2,}", d$Lemma_x[i])) {
d$head_x[i] <- tolower(unlist(strsplit(d$Lemma_x[i], " "))[grepl("[A-Z]{2,}", unlist(strsplit(d$Lemma_x[i], " ")))][1])
} else{
temp <- unlist(strsplit(d$Lemma_x[i], " "))
d$head_x[i] <- tolower(temp[length(temp)])
}
} else {
d$head_x[i] <- tolower(d$Lemma_x[i])
}
if(d$wordcount_y[i]>1) {
if(d$pos_y[i]!="NE" & grepl("[A-Z]{2,}", d$Lemma_y[i])) {
d$head_y[i] <- tolower(unlist(strsplit(d$Lemma_y[i], " "))[grepl("[A-Z]{2,}", unlist(strsplit(d$Lemma_y[i], " ")))][1])
} else{
temp <- unlist(strsplit(d$Lemma_y[i], " "))
d$head_y[i] <- tolower(temp[length(temp)])
}
} else {
d$head_y[i] <- tolower(d$Lemma_y[i])
}
}
# remove all with "unclear" -----------------------------------------------
# backup copy for subsequent analysis
d_backup <- d
d <- d[-which(d$concept_x=="unclear" | d$concept_y=="unclear"),]
# Chunk 7
# get hapaxes:
tibble(
types_x = length(unique(d$head_x)),
types_y = length(unique(d$head_y)),
types   = length(unique(paste0(d$head_x, "/", d$head_y))),
hapaxes_x = table(d$head_x) %>% as_tibble(.name_repair = "unique") %>% setNames(c("lemma_x", "n")) %>% filter(n == 1) %>% nrow(),
hapaxes_y = table(d$head_y) %>% as_tibble(.name_repair = "unique") %>% setNames(c("lemma_y", "n")) %>% filter(n == 1) %>% nrow(),
hapaxes_all = paste0(d$head_x, "/", d$head_y) %>% table %>% as_tibble() %>% setNames(c("lemma", "n")) %>% filter(n == 1) %>% nrow,
tokens = nrow(d)
) %>% kbl()
# Chunk 8: cnc
# network ----------------------------------------------------------------
d$concept_x <- factor(d$concept_x); d$concept_y <- factor(d$concept_y)
tbl <- d %>% select(concept_x, concept_y) %>% table %>% as.data.frame
tbl$number_x <- as.numeric(factor(tbl$concept_x))
tbl$number_y <- as.numeric(factor(tbl$concept_y))
# add a column in which the frequency is 0 if
# concept_x == concept_y
tbl$Freq_noself <- ifelse(tbl$concept_x == tbl$concept_y, NA, tbl$Freq)
# sort factors by frequency in concept_x ----------------------------------
conc_by_freq <- d$concept_x %>% table %>% sort(decreasing = T) %>% rownames()
tbl$concept_x <- factor(tbl$concept_x, levels = conc_by_freq)
tbl$concept_y <- factor(tbl$concept_y, levels = conc_by_freq)
# heatmaps ----------------------------------------------------------------
tbl %>% ggplot(aes(x = concept_x, y = concept_y, fill = log0(Freq))) +
geom_tile() + scale_fill_gradient(low = "yellow", high = "darkred") +
theme(axis.text.x = element_text(angle=45, hjust=.9)) +
guides(fill = guide_legend(title = "LogFreq"))
( p1 <- tbl %>% filter(Freq > 0) %>% ggplot(aes(x = concept_x, y = concept_y, fill = log0(Freq), label = Freq)) +
geom_tile() + scale_fill_gradient(low = "yellow", high = "darkred") +
guides(fill = guide_legend(title = "LogFreq")) + theme_classic() +
theme(axis.text.x = element_text(angle=45, hjust=.9)) +
geom_text(col = ifelse(log(filter(tbl, Freq > 0)$Freq > 6), "black", "white"), size = 4) +
theme(axis.text = element_text(size = 18)) +
theme(axis.title = element_text(size = 18)) +
theme(strip.text = element_text(size = 18)) +
theme(legend.text = element_text(size = 18)) +
theme(legend.title = element_text(size = 18, face = "bold")) +
theme(text = element_text(size = 18))
) + guides(fill = 'none')
ggsave("heatmap_x_is_the_new_y.png", height = 5, width = 6)
( p2 <- tbl %>% filter(Freq > 0) %>%
ggplot(aes(x = concept_x, y = concept_y, fill = log0(Freq_noself), label = Freq_noself)) +
geom_tile() + scale_fill_gradient(low = "yellow", high = "darkred") +
guides(fill = guide_legend(title = "LogFreq")) + theme_classic() +
theme(axis.text.x = element_text(angle=45, hjust=.9)) +
geom_text(col = ifelse(log(filter(tbl, Freq > 0)$Freq_noself > 6), "black", "white"), size = 4) +
theme(axis.text = element_text(size = 18)) +
theme(axis.title = element_text(size = 18)) +
theme(strip.text = element_text(size = 18)) +
theme(legend.text = element_text(size = 18)) +
theme(legend.title = element_text(size = 18, face = "bold")) +
theme(text = element_text(size = 18)) + guides(fill = "none") )
# Chunk 10: lmtz2
# re-import lemmatized lists ----------------------------------------------
l <- read_csv("../data/lemmatization.csv")
# Chunk 11: frqs
# get frequency of x & y lemmas -----------------------------------------------
lx <- left_join(tibble(word = d$head_x),
l) %>% na.omit %>% select(lemma) %>% table %>% as.data.frame(stringsAsFactors = F) %>% setNames(c("lemma", "Freq"))
ly <- left_join(tibble(word = d$head_y),
l) %>% na.omit %>% select(lemma) %>% table %>% as.data.frame(stringsAsFactors = F) %>% setNames(c("lemma", "Freq"))
# Chunk 12: encow
encow <- read_csv("../data/x_is_the_new_y_encow_frequencies.csv")
# Chunk 13: frqs2
# add frequencies -----------------------------------------------
lx$Freq_encow <- sapply(1:nrow(lx), function(i) sum(encow[grepw(lx$lemma[i], encow$word),]$Freq_encow))
ly$Freq_encow <- sapply(1:nrow(ly), function(i) sum(encow[grepw(ly$lemma[i], encow$word),]$Freq_encow))
# Chunk 14: discardzeros
# omit all that are not attested in ENCOW
lx <- filter(lx, Freq_encow > 0)
ly <- filter(ly, Freq_encow > 0)
collex(lx, corpsize = 1967348249)
collex(lx, corpsize = 1967348249, am = "odds")
collex(lx, corpsize = 1967348249, delta.p = T)
collex(lx, corpsize = 1967348249, am = "odds")
collex(lx, corpsize = 1967348249, am = "odds")  %>% head
select(collex(lx, corpsize = 1967348249, am = "odds"), COLLEX, COLL.STR.ODDS)
left_join(collex(lx, corpsize = 1967348249, delta.p = T),
select(collex(lx, corpsize = 1967348249, am = "odds"), COLLEX, COLL.STR.ODDS))
left_join(collex(lx, corpsize = 1967348249, delta.p = T),
select(collex(lx, corpsize = 1967348249, am = "odds"), COLLEX, COLL.STR.ODDS)) %>% datatable() %>% formatSignif(columns = c("OBS", "EXP", "COLL.STR.LOGL"), digits=5)
left_join(collex(ly, corpsize = 1967348249, delta.p = T),
select(collex(ly, corpsize = 1967348249, am = "odds"), COLLEX, COLL.STR.ODDS)) %>% datatable() %>% formatSignif(columns = c("OBS", "EXP", "COLL.STR.LOGL"), digits=5)
