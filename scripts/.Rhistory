filter(n > 20) %>%
rename(word1 = item1, word2 = item2) %>%
left_join(unigram_probs %>%
select(word1 = word, p1 = p),
by = "word1") %>%
left_join(unigram_probs %>%
select(word2 = word, p2 = p),
by = "word2") %>%
mutate(p_together = p / p1 / p2)
?widyr
normalized_prob[2005:2010,]
install.packages("irlbar")
install.packages("irlba")
library(irlba)
pmi_matrix <- normalized_prob %>%
mutate(pmi = log10(p_together)) %>%
cast_sparse(word1, word2, pmi)
#remove missing data
pmi_matrix@x[is.na(pmi_matrix@x)] <- 0
#run SVD
pmi_svd <- irlba(pmi_matrix, 256, maxit = 500)
install.packages("keras")
library(reticulate)
library(purrr)
library(text2vec)
install.packages("text2vec")
install.packages("Rtsne")
library(text2vec)
library(Rtsne)
library(plotly)
library(stringr)
library(keras)
# install Keras
install_keras()
library(tidyverse)
library(tidytext)
library(word2vec)
library(widyr)
library(irlba)
library(reticulate)
library(purrr)
library(text2vec)
library(Rtsne)
library(plotly)
library(stringr)
library(keras)
load(url("https://cbail.github.io/Elected_Official_Tweets.Rdata"))
# We want to use original tweets, not retweets:
elected_no_retweets <- elected_official_tweets %>%
filter(is_retweet == F) %>%
select(c("screen_name", "text"))
# Many tweets contain URLs, which we don't want considered in the model:
elected_no_retweets$text <- str_replace_all(string = elected_no_retweets$text,
pattern = "https.+",
replacement = "")
# tokenize text
tokenizer <- text_tokenizer(num_words = 20000)
tokenizer %>% fit_text_tokenizer(elected_no_retweets$text)
skipgrams_generator <- function(text, tokenizer, window_size, negative_samples) {
gen <- texts_to_sequences_generator(tokenizer, sample(text))
function() {
skip <- generator_next(gen) %>%
skipgrams(
vocabulary_size = tokenizer$num_words,
window_size = window_size,
negative_samples = 1
)
x <- transpose(skip$couples) %>% map(. %>% unlist %>% as.matrix(ncol = 1))
y <- skip$labels %>% as.matrix(ncol = 1)
list(x, y)
}
}
# Number of Dimensions in the embedding vector.
embedding_size <- 128
# Size of context window
skip_window <- 5
# Number of negative examples to sample for each word.
num_sampled <- 1
input_target <- layer_input(shape = 1)
input_context <- layer_input(shape = 1)
embedding <- layer_embedding(
input_dim = tokenizer$num_words + 1,
output_dim = embedding_size,
input_length = 1,
name = "embedding"
)
target_vector <- input_target %>%
embedding() %>%
layer_flatten()
context_vector <- input_context %>%
embedding() %>%
layer_flatten()
dot_product <- layer_dot(list(target_vector, context_vector), axes = 1)
output <- layer_dense(dot_product, units = 1, activation = "sigmoid")
model <- keras_model(list(input_target, input_context), output)
model %>% compile(loss = "binary_crossentropy", optimizer = "adam")
summary(model)
model %>%
fit_generator(
skipgrams_generator(elected_no_retweets$text, tokenizer, skip_window, negative_samples),
steps_per_epoch = 100, epochs = 2
)
model %>%
fit_generator(
skipgrams_generator(elected_no_retweets$text, tokenizer, skip_window, negative_samples),
steps_per_epoch = 100, epochs = 10
)
?fit_generator
fit_generator()
?fit()
model %>%
fit(
skipgrams_generator(elected_no_retweets$text, tokenizer, skip_window, negative_samples),
steps_per_epoch = 100, epochs = 10
)
?`keras-package`
version("keras")
str(model)
model
fit(model, skipgrams_generator(elected_no_retweets$text, tokenizer, skip_window, negative_samples),
steps_per_epoch = 100, epochs = 10
)
elected_no_retweets
fit(model, skipgrams_generator(as.data.frame(elected_no_retweets$text), tokenizer, skip_window, negative_samples),
steps_per_epoch = 100, epochs = 10)
fit(model, skipgrams_generator(as.data.frame(elected_no_retweets$text), tokenizer, skip_window, negative_samples),
steps_per_epoch = 100, epochs = 10)
skipgrams_generator(elected_no_retweets$text, tokenizer, skip_window, negative_samples)
skipgrams_generator(elected_no_retweets$text, tokenizer, skip_window, negative_samples)
?generator_next
?texts_to_sequences_generator
skipgrams_generator <- function(text, tokenizer, window_size, negative_samples) {
gen <- texts_to_sequences_generator(tokenizer, sample(text))
function() {
skip <- generator_next(gen) %>%
skipgrams(
vocabulary_size = tokenizer$num_words,
window_size = window_size,
negative_samples = 1
)
x <- transpose(skip$couples) %>% map(. %>% unlist %>% as.matrix(ncol = 1))
y <- skip$labels %>% as.matrix(ncol = 1)
list(x, y)
}
}
rm(list = ls())
library(tidyverse)
library(tidytext)
52*70
5950-350
40*0.65
46605/2
23303*2
46605*2
50*300
30*300
190*200
170*200
38700/2
667 + 725
36*300
mean(c(35,65))
50*300
1800+1150+300
275250*(0.25)
68812.5+226341+51926
68812.5+226341+51926+12000
32+26+5+2
6293.21*3
(93210*2)
(93210*4)
124005*4
496020/2
data("mtcars")
mtcars
7000*0.04
47+1
47+14
600/5
230-150
73+65
# Chunk 1: setup
knitr::opts_chunk$set(echo = TRUE)
assignInNamespace("cedta.pkgEvalsUserCode", c(data.table:::cedta.pkgEvalsUserCode,"rtvs"), "data.table")
# Chunk 2: sessionInfo
sessionInfo()
# Chunk 3: packages
# install CRAN packages (if not yet installed)
sapply(c("data.table", "tidyverse", "devtools", "readxl", "kableExtra", "ngram", "networkD3", "igraph", "network", "patchwork", "koRpus", "pbapply", "tidytext", "cluster", "ggrepel", "animation", "vroom", "ggrepel", "Rtsne", "DT"), function(x) if(!is.element(x, installed.packages())) install.packages(x, dependencies = T, repos = "http://cran.us.r-project.org"))
# install non-CRAN packages (if not yet installed)
if(!is.element("concordances", installed.packages())) {
devtools::install_github("hartmast/concordances")
}
# if this doesn't work, check sfla.ch for the package
if(!is.element("collostructions", installed.packages())) {
install.packages("https://sfla.ch/wp-content/uploads/2021/02/collostructions_0.2.0.tar.gz", repos = NULL)
}
# install "concordances" if not yet installed
if(!is.element("concordances", installed.packages())) {
devtools::install_github("hartmast/concordances", ref = "f4ca785
")
}
# install "wordVectors" if not yet installed
if(!is.element("wordVectors", installed.packages())) {
devtools::install_github("bmschmidt/wordVectors")
}
# load "concordances"
library(concordances)
library(tidyverse)
library(readxl)
library(data.table)
library(kableExtra)
library(collostructions)
library(wordVectors)
library(vroom)
library(ggrepel)
library(cluster)
library(patchwork)
library(DT)
# Chunk 5: import
d <- getNSE("../data/mother_of_all.xml", xml = TRUE, context_tags = FALSE, verbose = FALSE)
setwd("~/sciebo/Projekte/snowclones/Snowclones_paper_supplements/scripts")
# Chunk 1: setup
knitr::opts_chunk$set(echo = TRUE)
assignInNamespace("cedta.pkgEvalsUserCode", c(data.table:::cedta.pkgEvalsUserCode,"rtvs"), "data.table")
# Chunk 2: sessionInfo
sessionInfo()
# Chunk 3: packages
# install CRAN packages (if not yet installed)
sapply(c("data.table", "tidyverse", "devtools", "readxl", "kableExtra", "ngram", "networkD3", "igraph", "network", "patchwork", "koRpus", "pbapply", "tidytext", "cluster", "ggrepel", "animation", "vroom", "ggrepel", "Rtsne", "DT"), function(x) if(!is.element(x, installed.packages())) install.packages(x, dependencies = T, repos = "http://cran.us.r-project.org"))
# install non-CRAN packages (if not yet installed)
if(!is.element("concordances", installed.packages())) {
devtools::install_github("hartmast/concordances")
}
# if this doesn't work, check sfla.ch for the package
if(!is.element("collostructions", installed.packages())) {
install.packages("https://sfla.ch/wp-content/uploads/2021/02/collostructions_0.2.0.tar.gz", repos = NULL)
}
# install "concordances" if not yet installed
if(!is.element("concordances", installed.packages())) {
devtools::install_github("hartmast/concordances", ref = "f4ca785
")
}
# install "wordVectors" if not yet installed
if(!is.element("wordVectors", installed.packages())) {
devtools::install_github("bmschmidt/wordVectors")
}
# load "concordances"
library(concordances)
library(tidyverse)
library(readxl)
library(data.table)
library(kableExtra)
library(collostructions)
library(wordVectors)
library(vroom)
library(ggrepel)
library(cluster)
library(patchwork)
library(DT)
# Chunk 5: import
d <- getNSE("../data/mother_of_all.xml", xml = TRUE, context_tags = FALSE, verbose = FALSE)
# Chunk 6: anno
# write_excel_csv(d, "mother_of_all_ENCOW.csv")
d <- read_xlsx("../data/mother_of_all_ENCOW.xlsx")
# Chunk 7: dw
d <- filter(d, keep == "y")
# Chunk 8
# frequency table
d_tbl <- d %>% select(lemma) %>% table %>% as_tibble() %>% rename(c(Freq = "n")) %>% arrange(desc(Freq))
# overview table
tibble(
# types
Types = nrow(d_tbl),
# tokens
Tokens = sum(d_tbl$Freq),
# hapax legomena
"Hapax Legomena" = length(which(d_tbl$Freq==1))
) %>% kbl() %>% kable_material(c("striped", "hover"))
# Chunk 13
# re-import
d_tbl <- read_csv("../data/mother_of_all_with_encow_frequencies.csv")
# Chunk 14: excludethis
# omit one case where corpus frequency is
# smaller than cxn frequency
d_tbl <- subset(d_tbl, d_tbl[,2] <= d_tbl[,3])
# Chunk 15: collex
# sum(encow$Freq): 1805183579
# perform collexeme analysis
left_join(collex(as.data.frame(d_tbl), corpsize = 1805183579, delta.p = TRUE),
select(collex(as.data.frame(d_tbl), corpsize = 1805183579, am = "odds"), COLLEX, COLL.STR.ODDS)) %>% DT::datatable()  #kbl() %>%
#kable_material(c("striped", "hover")) %>% scroll_box(width = "800px", height = "200px")
# Chunk 16
# read data
moa <- read_xlsx("../data/motherofall_COCA.xlsx")
# full frequency list cannot be shared publicly
# for license reasons, hence we work with the
# list containing only the lemmas occurring in
# mother of all
#coca <- fread("../coca_2017_lemma_frequency_list.txt", quote = "")
coca <- fread("../data/coca_moa_lemma_frequencies.csv")
# replace whitespaces in column names
colnames(moa) <- gsub(" ", "_", colnames(moa))
colnames(coca) <- c("No", "Lemma", "Freq")
# omit false hits
moa <- subset(moa, keep=="y")
# types, tokens, and hapax legomena overall
moa_tbl1 <- moa %>% select(lemma) %>% table %>% as_tibble() %>% rename(c(Freq = "n") ) %>% arrange(desc(Freq))
tibble(
Tokens = sum(moa_tbl1$Freq),
Types = nrow(moa_tbl1),
"Hapax Legomena" = length(which(moa_tbl1$Freq==1))
)
# generate input for collostructional analysis
moa_lemmas <- moa$lemma %>% table %>% sort(decreasing = T) %>% as.data.frame(stringsAsFactors = F)
colnames(moa_lemmas) <- c("Lemma", "Freq_in_cxn")
all_lemmas <- coca[,Lemma, Freq]
setcolorder(all_lemmas, c("Lemma", "Freq"))
collex_input <- join.freqs(moa_lemmas, as.data.frame(all_lemmas), all = F)
colnames(collex_input) <- c("Lemma", "cxn_freq", "cxn_all")
collex_input <- subset(collex_input, cxn_all != 0)
left_join(collex(collex_input, corpsize = sum(coca$Freq), delta.p = T),
select(collex(collex_input, corpsize = sum(coca$Freq), am = "odds"), COLLEX, COLL.STR.ODDS)) %>% DT::datatable()
# %>% write_excel_csv("simple_collexeme_analysis.csv")
# relative frequency ------------------------------------------------------
# get COCA frequencies
coca_freq <- read_xlsx("../data/COCA2017_total_frequencies.xlsx")
# tabulate mother frequency
moa_tbl <- table(moa$Year) %>% as.data.frame(stringsAsFactors = F)
colnames(moa_tbl) <- c("YEAR", "Freq")
moa_tbl$YEAR <- as.numeric(moa_tbl$YEAR)
moa_tbl <- left_join(coca_freq, moa_tbl, by = "YEAR")
moa_tbl$pmw <- (moa_tbl$Freq / moa_tbl$TOTAL) * 1e06
# plot
# png("mother_of_all_coca_freq.png", width = 6.5, height = 5, un = "in", res = 300)
plot(moa_tbl$YEAR, moa_tbl$pmw, pch = 20, col = "blue",
ylab = "Frequency per million words", xlab = "Year",
main = "Token and type frequencies")
#main = expression(paste("[", italic("mother of all"), " X], COCA")))
abline(lm(moa_tbl$pmw ~ moa_tbl$YEAR), lty = 2, col = "darkgrey")
# dev.off()
# types per decade
moa_types <- moa %>% group_by(Decade) %>% summarise(
types = length(unique(lemma)),
n = n()
)
# types per year
moa_types_year <- moa %>% group_by(Year) %>% summarise(
types = length(unique(lemma))
)
# add to table with total frequencies
moa_tbl <- left_join(moa_types_year, moa_tbl, by = c("Year" = "YEAR"))
moa_tbl <- rename(moa_tbl, "YEAR" = "Year")
moa_tbl$types_pmw <- (moa_tbl$types / moa_tbl$TOTAL) * 1e6
# coca_freq per dcade
coca_freq$Decade <- floor(coca_freq$YEAR/10)*10
coca_freq_decade <- coca_freq %>% group_by(Decade) %>% summarise(
n = sum(TOTAL)
)
# types per decade
moa_types <- left_join(moa_types, coca_freq_decade)
moa_types$rel <- moa_types$types / moa_types$n
moa_types$rel %>% plot
moa_types$ttr <- moa_types$types / moa_types$n
# distribution of hapaxes
hapaxes <- table(moa$lemma) %>% as.data.frame %>% filter(Freq==1) %>% select(Var1) %>% as.vector
hapaxes <- as.character(hapaxes$Var1)
moa$hapax <- ifelse(moa$lemma %in% hapaxes, "y", "n")
moa_hapaxes <- moa %>% group_by(Decade) %>% summarise(
hapaxes = length(which(hapax=="y")),
n = n()
)
# plot potential productivity
moa_hapaxes$pp <- moa_hapaxes$hapaxes / moa_hapaxes$n
par(mfrow = c(1,3))
#png("types_tokens_mother_COCA.png", width = 12, height = 4, un = "in", res = 300)
par(mfrow=c(1,3))
par(mar = c(5.1, 5.1, 5.1, 2.1))
plot(moa_tbl$YEAR, moa_tbl$pmw, pch = 20, col = "blue",
ylab = "Frequency per million words", xlab = "Year",
main = "Token and type frequencies",
#main = expression(paste(bold("["), bolditalic("mother of all"), bold(" X], COCA"))),
cex = 2, cex.lab = 2, cex.axis=1.5)
abline(lm(moa_tbl$pmw ~ moa_tbl$YEAR), lty = 2, col = "darkgrey", lwd = 2)
points(moa_tbl$YEAR, moa_tbl$types_pmw, col = rgb(1,0,0,.5), pch = 18, cex = 1.3)
plot(moa_types$Decade, moa_types$ttr,
type = "b", pch=18,
ylab = "Types / Tokens", xlab = "Decade",
main = "Type-Token Ratio, COCA",
lwd = 2, col = "blue", cex = 2, cex.lab = 2, cex.axis=2,
xaxt = "n"
)
axis(1, at = c(1990, 2000, 2010), cex.axis=2)
plot(moa_hapaxes$Decade, moa_hapaxes$pp, type = "b", pch=18,
ylab = "Proportion hapaxes", xlab = "Decade",
main = "Potential productivity \n (proportion of hapax legomena), COCA",
lwd = 2, col = "blue", cex = 2, cex.lab = 2, cex.axis=2, xaxt = "n")
axis(1, at = c(1990, 2000, 2010), cex.axis=2)
# dev.off()
par(mar = c(5.1, 4.1, 4.1, 2.1))
par(mfrow=c(1,1))
# Chunk 17
(p1 <- collex(collex_input, corpsize = sum(coca$Freq), am = "odds") %>% ggplot(aes(x = log1p(OBS), y = log1p(COLL.STR.ODDS), label = COLLEX, col = log1p(OBS))) + geom_text() + theme_bw() + xlab("Log odds ratio") + ylab("Log Frequency") + scale_color_continuous(low = rgb(0,.7,1,.4), high = "black") + guides(col = 'none') + ggtitle("COCA") +theme(plot.title = element_text(face = "bold", hjust = 0.5)))
(p2 <- collex(as.data.frame(d_tbl), corpsize = 1805183579, am = "odds") %>% ggplot(aes(x = log1p(OBS), y = log1p(COLL.STR.ODDS), label = COLLEX, col = log1p(OBS))) + geom_text() + theme_bw() + xlab("Log odds ratio") + ylab("Log Frequency") + scale_color_continuous(low = rgb(0,.7,1,.4), high = "black") + guides(col = 'none') + ggtitle("ENCOW") + theme(plot.title = element_text(face = "bold", hjust = 0.5)))
p1 | p2
# ggsave("collex_moa_coca_encow.png", height = 7, width = 13)
# Chunk 19: distsem
# import the model
model <- readRDS("/Users/stefanhartmann/sciebo/Projekte/snowclones/word2vec/model.Rds")
# matrix of terms occurring in [mother of all X]'s open slot
cosine_dist_matrix <- cosineDist(model[[moa_lemmas$Lemma, average = FALSE]], model[[moa_lemmas$Lemma, average = FALSE]])
# multidimensional scaling
cosine_dists <- cosine_dist_matrix %>% cmdscale() %>% as.data.frame() %>% rownames_to_column() %>% setNames(c("lemma", "V1", "V2"))
# alternative: t-SNE
cosine_rtsne <- cosine_dist_matrix %>% Rtsne::Rtsne()
# we use Partitioning Around Medioids (PAM) to
# identify a small number of clusters (here: 3).
# As the results are not really meaningful, we have
# refrained from including it in the final analysis though.
# get PAM clusters
# for(i in 2:10) {
#   print(pam(cosine_dists, i)$silinfo$avg.width)
# }
#
pams <- pam(cosine_dists, 3)$clustering
# add frequency information
moa_freqs <- moa$lemma %>% table %>% as_tibble() %>% setNames(c("lemma", "n"))
# combine with MDS results
cosine_dists <- left_join(cosine_dists, moa_freqs)
# add log frequency
cosine_dists$LogFreq <- log1p(cosine_dists$n)
# 3 clusters
cosine_dists$clusters <- pams
# add Rtsne
cosine_dists <- cbind(cosine_dists, setNames(as.data.frame(cosine_rtsne$Y), c("dim1", "dim2")))
# visualize
# add one column that only serves to increase the font
# size of the remaining items (only for print version)
cosine_dists <- rbind(cosine_dists,
data.frame(lemma = "",
V1 = 0,
V2 = 0,
n = 0,
LogFreq = 0.3,
clusters = 1,
dim1 = 0, dim2 = 0))
set.seed(1994)
ggplot(cosine_dists, aes(x = V1, y = V2, label = lemma, size = LogFreq, col = factor(clusters))) +
geom_text_repel(max.overlaps = 15) +
guides(col = "none", size = "none") + theme_bw() +
# theme(axis.text = element_text(size = 18)) +
# theme(axis.title = element_text(size = 18)) +
# theme(strip.text = element_text(size = 18)) +
# theme(legend.title = element_text(size = 18, face = "bold")) +
# theme(text = element_text(size = 18)) +
scale_color_viridis_d() + ylab("dim2") + xlab("dim1")
# ggsave("distsem_moa_word2vec.png", width = 7, height = 6, dpi=500)
# use RTsne instead
set.seed(1994)
ggplot(cosine_dists, aes(x = dim1, y = dim2, label = lemma, size = LogFreq)) +
geom_text_repel(max.overlaps = 15) +
guides(size = "none") + theme_bw() +
# theme(axis.text = element_text(size = 18)) +
# theme(axis.title = element_text(size = 18)) +
# theme(strip.text = element_text(size = 18)) +
# theme(legend.title = element_text(size = 18, face = "bold")) +
# theme(text = element_text(size = 18)) +
scale_color_viridis_d() + ylab("dim2") + xlab("dim1")
# ggsave("distsem_moa_word2vec_tsne.png", width = 7, height = 6, dpi=500)
mother <- cosineDist(model[[c("mother", moa_lemmas$Lemma), average = FALSE]], model[[c("mother", moa_lemmas$Lemma), average = FALSE]])
mother <- as.data.frame(mother)
# png("mothervsrest.png", width = 6.5, height = 5, un = "in", res = 300)
mother[,which(colnames(mother)=="mother")] %>% hist(main = expression(paste("Cosine distance between ", italic("mother "), "and all X items")))
l <- list()
for(i in 1:100) {
spl <- sample(rownames(model), 1000)
l[[i]] <-cosineDist(model[[spl, average = F]], model[[spl, average = F]])
}
l_mean <- Reduce("+", l) / length(l)
l_sd   <- pbapply::pbapply(simplify2array(l), 1:2, sd)
tibble(mean = l_mean, sd = l_sd)
tibble(mean = l_mean, sd = l_sd) + geom_bar(aes( x = l_mean))
tibble(mean = l_mean, sd = l_sd) %>% ggplot() + geom_bar(aes( x = l_mean))
tibble(mean = l_mean, sd = l_sd) %>% ggplot() + geom_bar(aes( x = mean ))
tibble(mean = l_mean, sd = l_sd) %>% ggplot(aes(x = mean))
tibble(mean = l_mean, sd = l_sd)
l_mean
tibble(mean = unlist(l_mean), sd = unlist(l_sd))
tibble(mean = unlist(unname(l_mean)), sd = unlist(unname(l_sd)))
unlist(unname(l_mean))
as_vector(unlist(unname(l_mean)))
unlist(unlist(unname(l_mean)))
unlist(unlist(unname(l_mean))) %>% str
hist(l_mean)
hist(l_mean)
max(l_mean)
hist(l_mean)
l[[1]]
hist(l[[1]])
hist(l[[2]])
hist
max(l)
max(unlist(l))
max(unlist(l))
hist(l_mean)
saveRDS(mean, "mean.Rds")
saveRDS(l, "mean.Rds")
#saveRDS(l, "mean.Rds")
l <- readRDS("mean.Rds")
l_mean <- Reduce("+", l) / length(l)
hist(l_mean)
max(unlist(l))
