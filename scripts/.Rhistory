# install CRAN packages (if not yet installed)
sapply(c("data.table", "tidyverse", "devtools", "readxl", "kableExtra", "ngram", "networkD3", "igraph", "network", "patchwork", "koRpus", "pbapply", "tidytext", "cluster", "ggrepel", "animation"), function(x) if(!is.element(x, installed.packages())) install.packages(x, dependencies = T))
# install non-CRAN packages (if not yet installed)
if(!is.element("concordances", installed.packages())) {
devtools::install_github("hartmast/concordances")
}
# if this doesn't work, check sfla.ch for the package
if(!is.element("collostructions", installed.packages())) {
install.packages("https://sfla.ch/wp-content/uploads/2021/02/collostructions_0.2.0.tar.gz", repos = NULL)
}
# install "concordances" if not yet installed
if(!is.element("concordances", installed.packages())) {
devtools::install_github("hartmast/concordances", ref = "f4ca785
")
}
# load "concordances"
library(concordances)
library(tidyverse)
library(readxl)
library(data.table)
library(kableExtra)
library(collostructions)
d <- getNSE("../data/mother_of_all.xml", xml = TRUE, context_tags = FALSE, verbose = FALSE)
d <- getNSE("../data/mother_of_all.xml", xml = TRUE, context_tags = FALSE, verbose = FALSE)
# write_excel_csv(d, "mother_of_all_ENCOW.csv")
d <- read_xlsx("../data/mother_of_all_ENCOW.xlsx")
d <- filter(d, keep == "y")
# frequency table
d_tbl <- d %>% select(lemma) %>% table %>% as_tibble() %>% rename(c(lemma = ".", Freq = "n")) %>% arrange(desc(Freq))
tibble(
# types
Types = nrow(d_tbl),
# tokens
Tokens = sum(d_tbl$Freq),
# hapax legomena
"Hapax Legomena" = length(which(d_tbl$Freq==1))
) %>% kbl() %>% kable_material(c("striped", "hover"))
# re-import
d_tbl <- read_csv("../data/mother_of_all_with_encow_frequencies.csv")
# omit one case where corpus frequency is
# smaller than cxn frequency
d_tbl <- subset(d_tbl, d_tbl[,2] <= d_tbl[,3])
# perform collexeme analysis
collex(d_tbl, corpsize = 1312973874) %>% kbl() %>%
kable_material(c("striped", "hover")) %>% scroll_box(width = "800px", height = "200px")
# read data
moa <- read_xlsx("../data/motherofall_COCA.xlsx")
coca01 <- fread("../coca_2017_lemma_frequency_list.txt", quote = "")
coca01 <- fread("../../coca_2017_lemma_frequency_list.txt", quote = "")
coca01$V1
coca01$V2
coca01$V3
which(coca01$V1 %in% moa$lemma)
moa$lemma
which(coca01$V2 %in% moa$lemma)
coca01[which(coca01$V2 %in% moa$lemma),]
coca01[which(coca01$V2 %in% moa$lemma),] %>% write_excel_csv("../data/coca_moa_lemma_frequencies.csv")
#coca <- fread("../coca_2017_lemma_frequency_list.txt", quote = "")
coca <- fread("../data/coca_moa_lemma_frequencies.csv")
# replace whitespaces in column names
colnames(moa) <- gsub(" ", "_", colnames(moa))
colnames(coca) <- c("No", "Lemma", "Freq")
# omit false hits
moa <- subset(moa, keep=="y")
# types, tokens, and hapax legomena overall
moa_tbl1 <- moa %>% select(lemma) %>% table %>% as_tibble() %>% rename(c(lemma = ".", Freq = "n") ) %>% arrange(desc(Freq))
tibble(
Tokens = sum(moa_tbl1$Freq),
Types = nrow(moa_tbl1),
"Hapax Legomena" = length(which(moa_tbl1$Freq==1))
)
# generate input for collostructional analysis
moa_lemmas <- moa$lemma %>% table %>% sort(decreasing = T) %>% as.data.frame(stringsAsFactors = F)
colnames(moa_lemmas) <- c("Lemma", "Freq_in_cxn")
all_lemmas <- coca[,Lemma, Freq]
setcolorder(all_lemmas, c("Lemma", "Freq"))
collex_input <- join.freqs(moa_lemmas, as.data.frame(all_lemmas), all = F)
colnames(collex_input) <- c("Lemma", "cxn_freq", "cxn_all")
collex_input <- subset(collex_input, cxn_all != 0)
collex(collex_input, corpsize = sum(coca$Freq)) # %>% write_excel_csv("simple_collexeme_analysis.csv")
# get COCA frequencies
coca_freq <- read_xlsx("../COCA2017_total_frequencies.xlsx")
# get COCA frequencies
coca_freq <- read_xlsx("../../COCA2017_total_frequencies.xlsx")
coca_freq
# get COCA frequencies
coca_freq <- read_xlsx("../data/COCA2017_total_frequencies.xlsx")
# tabulate mother frequency
moa_tbl <- table(moa$Year) %>% as.data.frame(stringsAsFactors = F)
colnames(moa_tbl) <- c("YEAR", "Freq")
moa_tbl$YEAR <- as.numeric(moa_tbl$YEAR)
moa_tbl <- left_join(coca_freq, moa_tbl, by = "YEAR")
moa_tbl$pmw <- (moa_tbl$Freq / moa_tbl$TOTAL) * 1e06
# plot
# png("mother_of_all_coca_freq.png", width = 6.5, height = 5, un = "in", res = 300)
plot(moa_tbl$YEAR, moa_tbl$pmw, pch = 20, col = "blue",
ylab = "Frequency per million words", xlab = "Year",
main = expression(paste("[", italic("mother of all"), " X], COCA")))
abline(lm(moa_tbl$pmw ~ moa_tbl$YEAR), lty = 2, col = "darkgrey")
# types per decade
moa_types <- moa %>% group_by(Decade) %>% summarise(
types = length(unique(lemma)),
n = n()
)
# types per year
moa_types_year <- moa %>% group_by(Year) %>% summarise(
types = length(unique(lemma))
)
# add to table with total frequencies
moa_tbl <- left_join(moa_types_year, moa_tbl, by = c("Year" = "YEAR"))
moa_tbl <- rename(moa_tbl, "YEAR" = "Year")
moa_tbl$types_pmw <- (moa_tbl$types / moa_tbl$TOTAL) * 1e6
# coca_freq per dcade
coca_freq$Decade <- floor(coca_freq$YEAR/10)*10
coca_freq_decade <- coca_freq %>% group_by(Decade) %>% summarise(
n = sum(TOTAL)
)
# types per decade
moa_types <- left_join(moa_types, coca_freq_decade)
moa_types$rel <- moa_types$types / moa_types$n
moa_types$rel %>% plot
moa_types$ttr <- moa_types$types / moa_types$n
# distribution of hapaxes
hapaxes <- table(moa$lemma) %>% as.data.frame %>% filter(Freq==1) %>% select(Var1) %>% as.vector
hapaxes <- as.character(hapaxes$Var1)
moa$hapax <- ifelse(moa$lemma %in% hapaxes, "y", "n")
moa_hapaxes <- moa %>% group_by(Decade) %>% summarise(
hapaxes = length(which(hapax=="y")),
n = n()
)
# plot potential productivity
moa_hapaxes$pp <- moa_hapaxes$hapaxes / moa_hapaxes$n
par(mfrow = c(1,3))
#png("types_tokens_mother_COCA.png", width = 12, height = 4, un = "in", res = 300)
par(mfrow=c(1,3))
par(mar = c(5.1, 5.1, 5.1, 2.1))
plot(moa_tbl$YEAR, moa_tbl$pmw, pch = 20, col = "blue",
ylab = "Frequency per million words", xlab = "Year",
main = expression(paste(bold("["), bolditalic("mother of all"), bold(" X], COCA"))),
cex = 2, cex.lab = 2, cex.axis=1.5)
abline(lm(moa_tbl$pmw ~ moa_tbl$YEAR), lty = 2, col = "darkgrey", lwd = 2)
points(moa_tbl$YEAR, moa_tbl$types_pmw, col = rgb(1,0,0,.5), pch = 18, cex = 1.3)
plot(moa_types$Decade, moa_types$ttr,
type = "b", pch=18,
ylab = "Types / Tokens", xlab = "Decade",
main = "Type-Token Ratio, COCA",
lwd = 2, col = "blue", cex = 2, cex.lab = 2, cex.axis=1.5)
plot(moa_hapaxes$Decade, moa_hapaxes$pp, type = "b", pch=18,
ylab = "Proportion hapaxes", xlab = "Decade",
main = "Potential productivity \n (proportion of hapax legomena), COCA",
lwd = 2, col = "blue", cex = 2, cex.lab = 2, cex.axis=1.5)
# dev.off()
par(mar = c(5.1, 4.1, 4.1, 2.1))
par(mfrow=c(1,1))
# install CRAN packages (if not yet installed)
sapply(c("data.table", "tidyverse", "devtools", "readxl", "kableExtra", "ngram", "networkD3", "igraph", "network", "patchwork", "koRpus", "pbapply", "tidytext", "cluster", "ggrepel", "animation", "kableExtra", "DT"), function(x) if(!is.element(x, installed.packages())) install.packages(x, dependencies = T))
# install non-CRAN packages (if not yet installed)
if(!is.element("concordances", installed.packages())) {
devtools::install_github("hartmast/concordances")
}
# install non-CRAN packages (if not yet installed)
if(!is.element("wizard", installed.packages())) {
devtools::install_github("hartmast/wizard")
}
# if this doesn't work, check sfla.ch for the package
if(!is.element("collostructions", installed.packages())) {
install.packages("https://sfla.ch/wp-content/uploads/2021/02/collostructions_0.2.0.tar.gz", repos = NULL)
}
# load packages
library(readxl)
library(tidyverse)
library(ngram)
library(networkD3)
library(igraph)
library(network)
library(patchwork)
library(koRpus)
library(pbapply)
library(tidytext)
library(cluster)
library(ggrepel)
library(animation)
library(kableExtra)
library(DT)
library(collostructions) # available at sflach.ch
library(concordances) #available at github.com/hartmast/concordances
library(wizard) # available at github.com/hartmast/wizard
# logarithmize and return 0 instead of Inf if x==0
log0 <- function(x) {
x <- ifelse(x == 0, 0, log(x))
return(x)
}
# function for "prettyfying" df output
# inspired by https://github.com/rmcelreath/rethinking/blob/d0978c7f8b6329b94efa2014658d750ae12b1fa2/R/utilities.r
pretty_df <- function(df) {
# function for rounding
round_this <- function(x, digits = 2) ifelse(x < 1, signif(x, digits = digits), round(x, digits = 2))
# function for getting prettyfied dataframe
df_pretty <- as.data.frame(lapply(1:length(df),
function(i) if(!class(df[[i]]) %in% c("character", "factor"))
{
round_this(df[[i]])
} else {
return(df[[i]])
})
)
# set names to original names
colnames(df_pretty) <- colnames(df)
return(df_pretty)
}
# search for entire words
grepw <- function(pattern, x, perl = F, ...) {
grep(paste("^", pattern, "$", sep="", collapse=""), x, perl = perl, ...)
}
d <- read_xlsx("../data/ENCOW_x_is_the_new_y_without_false_hits.xlsx")
d <- filter(d, keep == "y")
d$wordcount_x <- sapply(1:nrow(d), function(i) wordcount(trimws(d$Lemma_x[i])))
d$wordcount_y <- sapply(1:nrow(d), function(i) wordcount(trimws(d$Lemma_y[i])))
# empty columns for heads
d$head_x <- NA; d$head_y <- NA
# add wordcount for x and y lemmas
d$wordcount_x <- sapply(1:nrow(d), function(i) wordcount(trimws(d$Lemma_x[i])))
d$wordcount_y <- sapply(1:nrow(d), function(i) wordcount(trimws(d$Lemma_y[i])))
# get heads
for(i in 1:nrow(d)) {
if(d$wordcount_x[i]>1) {
if(d$pos_x[i]!="NE" & grepl("[A-Z]{2,}", d$Lemma_x[i])) {
d$head_x[i] <- tolower(unlist(strsplit(d$Lemma_x[i], " "))[grepl("[A-Z]{2,}", unlist(strsplit(d$Lemma_x[i], " ")))][1])
} else{
temp <- unlist(strsplit(d$Lemma_x[i], " "))
d$head_x[i] <- tolower(temp[length(temp)])
}
} else {
d$head_x[i] <- tolower(d$Lemma_x[i])
}
if(d$wordcount_y[i]>1) {
if(d$pos_y[i]!="NE" & grepl("[A-Z]{2,}", d$Lemma_y[i])) {
d$head_y[i] <- tolower(unlist(strsplit(d$Lemma_y[i], " "))[grepl("[A-Z]{2,}", unlist(strsplit(d$Lemma_y[i], " ")))][1])
} else{
temp <- unlist(strsplit(d$Lemma_y[i], " "))
d$head_y[i] <- tolower(temp[length(temp)])
}
} else {
d$head_y[i] <- tolower(d$Lemma_y[i])
}
}
# backup copy for subsequent analysis
d_backup <- d
d <- d[-which(d$concept_x=="unclear" | d$concept_y=="unclear"),]
tibble(
types_x = length(unique(d$head_x)),
types_y = length(unique(d$head_y)),
types   = length(unique(paste0(d$head_x, "/", d$head_y))),
hapaxes_x = table(d$head_x) %>% as_tibble(.name_repair = "unique") %>% setNames(c("lemma_x", "n")) %>% filter(n == 1) %>% nrow(),
hapaxes_y = table(d$head_y) %>% as_tibble(.name_repair = "unique") %>% setNames(c("lemma_y", "n")) %>% filter(n == 1) %>% nrow(),
hapaxes_all = paste0(d$head_x, "/", d$head_y) %>% table %>% as_tibble() %>% setNames(c("lemma", "n")) %>% filter(n == 1) %>% nrow,
tokens = nrow(d)
) %>% kbl()
# network ----------------------------------------------------------------
d$concept_x <- factor(d$concept_x); d$concept_y <- factor(d$concept_y)
tbl <- d %>% select(concept_x, concept_y) %>% table %>% as.data.frame
tbl$number_x <- as.numeric(factor(tbl$concept_x))
tbl$number_y <- as.numeric(factor(tbl$concept_y))
# add a column in which the frequency is 0 if
# concept_x == concept_y
tbl$Freq_noself <- ifelse(tbl$concept_x == tbl$concept_y, NA, tbl$Freq)
conc_by_freq <- d$concept_x %>% table %>% sort(decreasing = T) %>% rownames()
tbl$concept_x <- factor(tbl$concept_x, levels = conc_by_freq)
tbl$concept_y <- factor(tbl$concept_y, levels = conc_by_freq)
tbl %>% ggplot(aes(x = concept_x, y = concept_y, fill = log0(Freq))) +
geom_tile() + scale_fill_gradient(low = "yellow", high = "darkred") +
theme(axis.text.x = element_text(angle=45, hjust=.9)) +
guides(fill = guide_legend(title = "LogFreq"))
( p1 <- tbl %>% filter(Freq > 0) %>% ggplot(aes(x = concept_x, y = concept_y, fill = log0(Freq), label = Freq)) +
geom_tile() + scale_fill_gradient(low = "yellow", high = "darkred") +
guides(fill = guide_legend(title = "LogFreq")) + theme_classic() +
theme(axis.text.x = element_text(angle=45, hjust=.9)) +
geom_text(col = ifelse(log(filter(tbl, Freq > 0)$Freq > 6), "black", "white"), size = 4) +
theme(axis.text = element_text(size = 18)) +
theme(axis.title = element_text(size = 18)) +
theme(strip.text = element_text(size = 18)) +
theme(legend.text = element_text(size = 18)) +
theme(legend.title = element_text(size = 18, face = "bold")) +
theme(text = element_text(size = 18))
)
( p2 <- tbl %>% filter(Freq > 0) %>%
ggplot(aes(x = concept_x, y = concept_y, fill = log0(Freq_noself), label = Freq_noself)) +
geom_tile() + scale_fill_gradient(low = "yellow", high = "darkred") +
guides(fill = guide_legend(title = "LogFreq")) + theme_classic() +
theme(axis.text.x = element_text(angle=45, hjust=.9)) +
geom_text(col = ifelse(log(filter(tbl, Freq > 0)$Freq_noself > 6), "black", "white"), size = 4) ) +
theme(axis.text = element_text(size = 18)) +
theme(axis.title = element_text(size = 18)) +
theme(strip.text = element_text(size = 18)) +
theme(legend.text = element_text(size = 18)) +
theme(legend.title = element_text(size = 18, face = "bold")) +
theme(text = element_text(size = 18))
# export heads for lemmatization ------------------------------------------
c(d %>% filter(!(concept_x=="person" & pos_x=="NE")) %>% select(head_x),
d %>% filter(!(concept_y=="person" & pos_y=="NE")) %>% select(head_y)) %>%
unlist %>% unique %>% as.data.frame
l <- read_csv("../data/lemmatization.csv")
lx <- left_join(tibble(word = d$head_x),
l) %>% na.omit %>% select(lemma) %>% table %>% as.data.frame(stringsAsFactors = F) %>% setNames(c("lemma", "Freq"))
ly <- left_join(tibble(word = d$head_y),
l) %>% na.omit %>% select(lemma) %>% table %>% as.data.frame(stringsAsFactors = F) %>% setNames(c("lemma", "Freq"))
encow <- read_csv("../data/x_is_the_new_y_encow_frequencies.csv")
# add frequencies -----------------------------------------------
lx$Freq_encow <- sapply(1:nrow(lx), function(i) sum(encow[grepw(lx$lemma[i], encow$word),]$Freq_encow))
ly$Freq_encow <- sapply(1:nrow(ly), function(i) sum(encow[grepw(ly$lemma[i], encow$word),]$Freq_encow))
# omit all that are not attested in ENCOW
lx <- filter(lx, Freq_encow > 0)
ly <- filter(ly, Freq_encow > 0)
collex.dist(lx) %>% datatable() %>% formatSignif(columns = c("E.CXN1", "E.CXN2", "COLL.STR.LOGL"), digits=3)
collex.dist(ly) %>% datatable() %>% formatSignif(columns = c("E.CXN1", "E.CXN2", "COLL.STR.LOGL"), digits=3)
d <- left_join(d, rename(l, c(lemma_head_x = lemma)), by = c("head_x" = "word"), all.x = T)
d <- left_join(d, rename(l, c(lemma_head_y = lemma)), by = c("head_y" = "word"), all.x = T)
# fill NA head columns
d$lemma_head_x <- ifelse(is.na(d$lemma_head_x), d$head_x, d$lemma_head_x)
d$lemma_head_y <- ifelse(is.na(d$lemma_head_y), d$head_y, d$lemma_head_y)
# collocates
coll <- read_csv("../data/X_is_the_new_Y_distsem/collocates.csv")
# "collocate" column to rownames:
coll <- as.data.frame(coll)
rownames(coll) <- coll$collocate
coll <- coll[ , -1] # remove first column
coll <- t(coll) # switch rows and columns
# get expected frequencies
coll <- as.matrix(coll)
coll.exp <- chisq.test(coll)$expected
coll.PMI <- log2(coll / coll.exp)
coll.PPMI <- ifelse(coll.PMI < 0, 0, coll.PMI)
# cosine similarity function
# (adopted from Levshina's [2015] Rling package)
nr <- nrow(coll)
m <- matrix(NA, nr, nr)
colnames(m) <- rownames(m) <- rownames(coll)
# export
# saveRDS(m, "m.Rds")
m <- readRDS("../data/X_is_the_new_Y_distsem/m.Rds")
# get distances
m2 <- 1 - (m / max(m[m<1]))
# backup copy
m2_matrix <- m2
# as.dist
m2 <- as.dist(m2)
# as matrix
m2_matrix <- as.matrix(m2, varnames = c("row", "col"))
# mds
m3 <- cmdscale(m2)
m3 <- rownames_to_column(as.data.frame(m3))
colnames(m3) <- c("Lemma", "dim1", "dim2")
# Clusters
m2_clust <- cluster::pam(m2, 14)
m2_cluster <- m2_clust$clustering %>% as.data.frame()
m2_cluster <- rownames_to_column(m2_cluster)
colnames(m2_cluster) <- c("Lemma", "Cluster")
# join dataframes
m3 <- left_join(m3, m2_cluster)
# add frequency information
m3 <- left_join(m3, l, by = c("Lemma" = "word"))
m3$freq_x <- sapply(1:nrow(m3), function(i) length(which(d$lemma_head_x == m3$lemma[i])))
m3$freq_y <- sapply(1:nrow(m3), function(i) length(which(d$lemma_head_y == m3$lemma[i])))
m3$freq <- m3$freq_x + m3$freq_y
# add relative frequency with which each item occurs in x or y slot
m3$rel_x <- m3$freq_x / (m3$freq_x + m3$freq_y)
m3$rel_y <- m3$freq_y / (m3$freq_x + m3$freq_y)
m3 <- replace_na(m3, list(rel_x = 0, rel_y = 0))
# set a seed so that the location of the datapoints
# (arranged by ggrepel package) will remain the same
set.seed(1985)
#plot
(p1 <- ggplot(filter(m3, freq >= 5), aes(x = dim1, y = dim2, label = Lemma, col = factor(Cluster))) +
geom_text_repel(aes(size = log1p(freq)*2), max.overlaps = 15) +
scale_color_discrete(terrain.colors(14)) +
guides(col = "none", size = "none") + theme_bw() + theme(axis.text = element_text(size = 18)) +
theme(axis.title = element_text(size = 18)) +
theme(strip.text = element_text(size = 18)) +
theme(legend.text = element_text(size = 18)) +
theme(legend.title = element_text(size = 18, face = "bold")) +
theme(text = element_text(size = 10)) )
# add Cosine distance to original dataframe
d$cosine_distance <- NA
for(i in 1:nrow(d)) {
if(d$lemma_head_x[i] %in% colnames(m2_matrix) &&
d$lemma_head_y[i] %in% rownames(m2_matrix)) {
d$cosine_distance[i] <- m2_matrix[which(colnames(m2_matrix) == d$lemma_head_x[i]),
which(rownames(m2_matrix) == d$lemma_head_y[i])]
}
}
# add column with x and y
d$lemma_heads <- character(nrow(d))
d$lemma_heads <- paste(d$lemma_head_x, d$lemma_head_y, sep = "/")
d %>% arrange(desc(cosine_distance)) %>%
select(lemma_head_x, lemma_head_y, cosine_distance) %>% na.omit %>%
unique %>% datatable() %>% formatSignif(columns = "cosine_distance", digits=3)
# add relative freqeuency in x and y slot
m3$rel_x <- m3$freq_x / (m3$freq_x + m3$freq_y)
m3$rel_y <- m3$freq_y / (m3$freq_x + m3$freq_y)
m3 <- replace_na(m3, list(rel_x = 0, rel_y = 0))
# set a seed so that the location of the datapoints
# (arranged by ggrepel package) will remain the same
set.seed(1985)
#plot
(p2 <- ggplot(filter(m3, freq >= 5), aes(x = dim1, y = dim2, label = Lemma, col = rel_x)) +
geom_text_repel(aes(size = log1p(freq)*2), max.overlaps = 15) +
scale_color_continuous(low = "blue", high = "red") +
guides(col = "none", size = "none") + theme_bw() + theme(axis.text = element_text(size = 18)) +
theme(axis.title = element_text(size = 18)) +
theme(strip.text = element_text(size = 18)) +
theme(legend.text = element_text(size = 18)) +
theme(legend.title = element_text(size = 18, face = "bold")) +
theme(text = element_text(size = 10)) )
distances <- d %>% arrange(desc(cosine_distance)) %>%
select(lemma_head_x, lemma_head_y, cosine_distance) %>% na.omit %>%
unique
distances %>% ggplot(aes(x = cosine_distance)) + geom_histogram(binwidth = 0.03, col = "black", fill = "grey50") + theme_classic() + ylab("Count") + xlab("Cosine distance")
find_items <- function(x, y) {
return(distances[which(distances$lemma_head_x == x & distances$lemma_head_y == y),])
}
rbind(find_items("anxiety", "depression"),
find_items("female", "male"),
find_items("democrat", "republican"),
find_items("abnormality", "disease"),
find_items("alpha", "beta"),
find_items("audio", "video"),
find_items("pear", "raspberry"),
find_items("sushi", "pizza"),
find_items("small", "large"),
find_items("environmentalist", "socialist"),
find_items("sugar", "nicotine"),
find_items("computer", "radio"),
find_items("publishing", "literacy"),
find_items("paper", "confidentiality"),
find_items("mean", "green"),
find_items("sustainable", "black"),
find_items("ethics", "green"),
find_items("funds", "black")
) %>% mutate(lemmas = factor(paste0(lemma_head_x, " - ", lemma_head_y), levels = paste0(lemma_head_x, " - ", lemma_head_y))) %>%
ggplot(aes(x = cosine_distance, y = lemmas)) + geom_col(fill = "black") + theme_bw() + ylab("Lemmas") + xlab("Cosine distance")
# read data ---------------------------------------------------------------
d <- read_xlsx("../data/COCA_X_is_are_the_new_Y.xlsx")
# add decade
d$Decade <- gsub("(?<=...).", "0", d$Year, perl = T)
# remove false hits
d <- filter(d, keep == "y")
# types, tokens, hapaxes
tibble(
tokens = nrow(d),
types_x  = d$lemma_x %>% unique %>% length,
types_y  = d$Lemma_y %>% unique %>% length,
types_all = paste0(d$lemma_x, "/", d$Lemma_y) %>% unique %>% length,
hapaxes_x = d$lemma_x %>% table %>% as_tibble() %>% filter(n == 1) %>% nrow,
hapaxes_y = d$Lemma_y %>% table %>% as_tibble() %>% filter(n == 1) %>% nrow,
hapaxes_all = paste0(d$lemma_x, "/", d$Lemma_y) %>% table %>% as_tibble() %>% filter(n == 1) %>% nrow
) %>% kbl()
# quick visualization -----------------------------------------------------
qbarplot(filter(d, black == "n"), Decade, concept_x, pos_x, wrap100 = T) +
scale_fill_grey(start = .8, end = .3)
qbarplot(filter(d, black == "n"), Decade, concept_x) +
scale_fill_grey(start = .8, end = .3)
qbarplot(filter(d, black == "n"), Decade, pos_x)
qbarplot(d, Decade, black)
# read total frequencies
coca <- read_xlsx("../COCA2017_total_frequencies.xlsx")
# read total frequencies
coca <- read_xlsx("../data/COCA2017_total_frequencies.xlsx")
# bin by decade
coca$Decade <- as.integer(gsub("(?<=...).", "0", as.character(coca$YEAR), perl = T))
coca_dec <- coca %>% group_by(Decade) %>% summarise(
Freq = sum(TOTAL)
)
# frequency of X is the new Y per decade
d_tbl <- table(d$Decade) %>% as.data.frame(stringsAsFactors = FALSE)
colnames(d_tbl) <- c("Decade", "Freq_x_is_the_new_y")
d_tbl$Decade <- as.integer(d_tbl$Decade)
d_tbl <- left_join(d_tbl, coca_dec, by = "Decade")
d_tbl$pmw <- (d_tbl$Freq_x_is_the_new_y / d_tbl$Freq) * 1e06
plot(d_tbl$Decade, d_tbl$pmw, ylim = c(0,0.25), type = "b")
